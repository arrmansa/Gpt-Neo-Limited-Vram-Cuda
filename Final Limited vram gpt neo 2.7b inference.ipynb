{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1b92bc",
   "metadata": {},
   "source": [
    "IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d35f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from transformers import AutoTokenizer#, GPTNeoForCausalLM,\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e873ce6",
   "metadata": {},
   "source": [
    "TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "925f73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb9f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Sampling settings\n",
    "#@markdown You can modify sampling settings here. Don't forget to run the cell again after changing. The number of generated tokens is subtracted from the context window size, don't set it high.\n",
    "top_k = 60 #@param {type:\"number\"}\n",
    "top_p = 0.7 #@param {type:\"number\"}\n",
    "temperature = 1#@param {type:\"number\"}\n",
    "number_generated_tokens =  48#@param {type:\"integer\"}\n",
    "repetition_penalty = 3.01 #@param {type:\"number\"}\n",
    "repetition_penalty_range = 1000 #@param {type:\"number\"}\n",
    "repetition_penalty_slope = 1 #@param {type:\"number\"}\n",
    "#@markdown Temperatures seem to give results different from those in AID, so play around with it. Even 0.5 can give good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe06492",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_prompt = \"test \" #@param {type:\"string\"}\n",
    "ids = tokenizer(basic_prompt*10000, return_tensors=\"pt\",truncation=True).input_ids\n",
    "n_ids = ids.shape[1]\n",
    "if n_ids < 1:\n",
    "  n_ids = 1\n",
    "  ids = torch.tensor([[tokenizer.eos_token_id]])\n",
    "max_length = n_ids + number_generated_tokens\n",
    "modelinputids = ids.long().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78dc527",
   "metadata": {},
   "source": [
    "LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4582a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import GPTNeoForCausalLM\\nmodel = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\").half()\\nwith open(\\'gptneo.pkl\\', \\'wb\\') as f:\\n    pickle.dump(model, f)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CODE TO SAVE MODEL:\n",
    "\"\"\"\n",
    "from transformers import GPTNeoForCausalLM\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\").half()\n",
    "with open('gptneo.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc52efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 40\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 2560)\n",
      "    (wpe): Embedding(2048, 2560)\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (12): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (13): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (14): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (15): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (16): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (17): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (18): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (19): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (20): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (21): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (22): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (23): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (24): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (25): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (26): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (27): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (28): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (29): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (30): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (31): GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoLocalSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0, inplace=False)\n",
      "            (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
      "            (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=2560, out_features=10240, bias=True)\n",
      "          (c_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"1\",gc.collect())\n",
    "#Pickle is just a saved gpt-neo 2.7B model (GPTNeoForCausalLM) , Used here because it has lower peak ram usage\n",
    "with open('gptneo.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "print(model.eval().half().to(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dd11d5",
   "metadata": {},
   "source": [
    "# Modify the forward function of GPTNeoModel\n",
    "Set the number_of_parts in new_forward. Recommend 4 for 8gb vram and 32 for 6gb vram. (4 may work on 6gb vram and 2 may work on 8 gb vram depending on os/cuda/etc.) <br> Speed difference between number of parts seems extremely minimal, indicating that ram->vram transfer is pretty much the only bottleneck of this process, and the gpu compute is almost instant in comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e66b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_parts = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b8bda8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoForCausalLM,GPTNeoModel\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b12c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_forward(\n",
    "    self,\n",
    "    input_ids=None,\n",
    "    past_key_values=None,\n",
    "    attention_mask=None,\n",
    "    token_type_ids=None,\n",
    "    position_ids=None,\n",
    "    head_mask=None,\n",
    "    inputs_embeds=None,\n",
    "    use_cache=None,\n",
    "    output_attentions=None,\n",
    "    output_hidden_states=None,\n",
    "    return_dict=None,\n",
    "):\n",
    "    global number_of_parts\n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    if input_ids is not None and inputs_embeds is not None:\n",
    "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "    elif input_ids is not None:\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        batch_size = input_ids.shape[0]\n",
    "    elif inputs_embeds is not None:\n",
    "        input_shape = inputs_embeds.size()[:-1]\n",
    "        batch_size = inputs_embeds.shape[0]\n",
    "    else:\n",
    "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
    "    if position_ids is not None:\n",
    "        position_ids = position_ids.view(-1, input_shape[-1])\n",
    "\n",
    "    if past_key_values is None:\n",
    "        past_length = 0\n",
    "        past_key_values = tuple([None] * len(self.h))\n",
    "    else:\n",
    "        past_length = past_key_values[0][0].size(-2)\n",
    "    if position_ids is None:\n",
    "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
    "        position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "\n",
    "    # Attention mask.\n",
    "    if attention_mask is not None:\n",
    "        assert batch_size > 0, \"batch_size has to be defined and > 0\"\n",
    "        global_attention_mask = attention_mask.view(batch_size, -1)\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        global_attention_mask = global_attention_mask[:, None, None, :]\n",
    "\n",
    "        # Since global_attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        global_attention_mask = global_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        global_attention_mask = (1.0 - global_attention_mask) * -10000.0\n",
    "    else:\n",
    "        global_attention_mask = None\n",
    "\n",
    "    # Prepare head mask if needed\n",
    "    # 1.0 in head_mask indicate we keep the head\n",
    "    # attention_probs has shape bsz x num_headss x N x N\n",
    "    # head_mask has shape n_layer x batch x num_headss x N x N\n",
    "    head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n",
    "\n",
    "    if inputs_embeds is None:\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "    position_embeds = self.wpe(position_ids)\n",
    "    hidden_states = inputs_embeds + position_embeds\n",
    "\n",
    "    if token_type_ids is not None:\n",
    "        token_type_embeds = self.wte(token_type_ids)\n",
    "        hidden_states = hidden_states + token_type_embeds\n",
    "\n",
    "    hidden_states = self.drop(hidden_states)\n",
    "\n",
    "    output_shape = input_shape + (hidden_states.size(-1),)\n",
    "\n",
    "    presents = () if use_cache else None\n",
    "    all_self_attentions = () if output_attentions else None\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "\n",
    "    for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
    "        if number_of_parts == 2:\n",
    "            if i == 0:\n",
    "                cudastreams = {}\n",
    "                for j in range(0,16):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j+16].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                        \n",
    "                for j in range(0,16):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                        \n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "            if i == 16:\n",
    "                cudastreams = {}\n",
    "                for j in range(16,32):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j-16].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                        \n",
    "                for j in range(16,32):  \n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                            pass\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "        if number_of_parts == 4:\n",
    "            if i == 0:\n",
    "                cudastreams = {}\n",
    "                for j in range(0,8):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j+24].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                for j in range(0,8):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "            if i == 8:\n",
    "                cudastreams = {}\n",
    "                for j in range(8,16):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j-8].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                for j in range(8,16):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                    \n",
    "            if i == 16:\n",
    "                cudastreams = {}\n",
    "                for j in range(16,24):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j-8].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                for j in range(16,24):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        model.transformer.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "            if i == 24:\n",
    "                cudastreams = {}\n",
    "                for j in range(24,32):\n",
    "                    cudastreams[j] = torch.cuda.Stream()\n",
    "                    for param1,param2 in zip(self.h[j].parameters(),self.h[j-8].parameters()):\n",
    "                        param1.data = param2.data\n",
    "                for j in range(24,32):\n",
    "                    with torch.cuda.stream(cudastreams[j]):\n",
    "                        for param1,param2 in zip(self.h[j].parameters(),self.extrastorage[j].parameters()):\n",
    "                            param1.data.copy_(param2.data, non_blocking=True)\n",
    "                        self.h[j].to(\"cuda\", non_blocking=True)\n",
    "                torch.cuda.synchronize()\n",
    "                del cudastreams\n",
    "                \n",
    "        if number_of_parts == 32:\n",
    "            \n",
    "            if i == 0:\n",
    "                for param1,param2 in zip(self.h[i].parameters(),self.h[31].parameters()):\n",
    "                    param1.data = param2.data\n",
    "                    \n",
    "                for param1,param2 in zip(self.h[0].parameters(),self.extrastorage[0].parameters()):\n",
    "                    param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "                self.h[0].to(\"cuda\", non_blocking=True)\n",
    "                    \n",
    "                    \n",
    "            if i >= 1:\n",
    "                for param1,param2 in zip(self.h[i].parameters(),self.h[i-1].parameters()):\n",
    "                    param1.data = param2.data\n",
    "                    \n",
    "                for param1,param2 in zip(self.h[i].parameters(),self.extrastorage[i].parameters()):\n",
    "                    param1.data.copy_(param2.data, non_blocking=True)\n",
    "                self.h[i].to(\"cuda\", non_blocking=True)\n",
    "        \n",
    "        attn_type = self.config.attention_layers[i]\n",
    "        attn_mask = global_attention_mask if attn_type == \"global\" else attention_mask\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
    "\n",
    "            if use_cache:\n",
    "                logger.warning(\n",
    "                    \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
    "                    \"`use_cache=False`...\"\n",
    "                )\n",
    "                use_cache = False\n",
    "\n",
    "            def create_custom_forward(module):\n",
    "                def custom_forward(*inputs):\n",
    "                    # None for past_key_value\n",
    "                    return module(*inputs, use_cache, output_attentions)\n",
    "\n",
    "                return custom_forward\n",
    "\n",
    "            outputs = torch.utils.checkpoint.checkpoint(\n",
    "                create_custom_forward(block),\n",
    "                hidden_states,\n",
    "                None,\n",
    "                attn_mask,\n",
    "                head_mask[i],\n",
    "            )\n",
    "        else:\n",
    "            outputs = block(\n",
    "                hidden_states,\n",
    "                layer_past=layer_past,\n",
    "                attention_mask=attn_mask,\n",
    "                head_mask=head_mask[i],\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "\n",
    "        hidden_states = outputs[0]\n",
    "        if use_cache is True:\n",
    "            presents = presents + (outputs[1],)\n",
    "\n",
    "        if output_attentions:\n",
    "            all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
    "\n",
    "    hidden_states = self.ln_f(hidden_states)\n",
    "\n",
    "    hidden_states = hidden_states.view(*output_shape)\n",
    "    # Add last hidden state\n",
    "    if output_hidden_states:\n",
    "        all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "    if not return_dict:\n",
    "        return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n",
    "\n",
    "    return BaseModelOutputWithPast(\n",
    "        last_hidden_state=hidden_states,\n",
    "        past_key_values=presents,\n",
    "        hidden_states=all_hidden_states,\n",
    "        attentions=all_self_attentions,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "801e67f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function GPTNeoModel.forward at 0x000002198FE07708>\n",
      "<function new_forward at 0x0000021989774318>\n"
     ]
    }
   ],
   "source": [
    "print(GPTNeoModel.forward)\n",
    "print(new_forward)\n",
    "GPTNeoModel.forward = new_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4934e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function new_forward at 0x0000021989774318>\n"
     ]
    }
   ],
   "source": [
    "print(GPTNeoModel.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e8bda0",
   "metadata": {},
   "source": [
    "# Prepare model for ram-vram swaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce92ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval().to(\"cpu\")\n",
    "model.transformer.wte.to(\"cuda\")\n",
    "model.transformer.wpe.to(\"cuda\")\n",
    "model.transformer.ln_f.to(\"cuda\")\n",
    "model.lm_head.to(\"cuda\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a61cf6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.transformer.wte.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.transformer.wpe.parameters():\n",
    "    param.requires_grad = False\n",
    "for i in range(32):\n",
    "    for param in model.transformer.h[i].parameters():\n",
    "        param.requires_grad = False\n",
    "for param in model.transformer.ln_f.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.lm_head.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c28c52",
   "metadata": {},
   "source": [
    "# extra storage for model.transformer.h (will use extra 5gb ram temporarily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f683f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(model.transformer,\"extrastorage\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6ed7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.extrastorage = copy.deepcopy(model.transformer.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef24b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalltensor = torch.tensor(0).to(\"cuda\")\n",
    "for j in range(32):\n",
    "    for param1 in model.transformer.h[j].parameters():\n",
    "        param1.data = smalltensor\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d38804c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer.extrastorage.to(\"cpu\")\n",
    "for i in range(32):\n",
    "    for param in model.transformer.extrastorage[i].parameters():\n",
    "        param.requires_grad = False\n",
    "        param.data.pin_memory()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edbce66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_parts = 32\n"
     ]
    }
   ],
   "source": [
    "if number_of_parts == 2:\n",
    "    for j in range(16,32):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "            param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "        model.transformer.h[j].to(\"cuda\", non_blocking=True)  \n",
    "    print(\"number_of_parts = 4\" )\n",
    "    \n",
    "if number_of_parts == 4:\n",
    "    for j in range(24,32):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "            param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "        model.transformer.h[j].to(\"cuda\", non_blocking=True)  \n",
    "    print(\"number_of_parts = 4\" )\n",
    "    \n",
    "if number_of_parts == 32:\n",
    "    for param1,param2 in zip(model.transformer.h[31].parameters(),model.transformer.extrastorage[31].parameters()):\n",
    "        param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "    model.transformer.h[31].to(\"cuda\", non_blocking=True)  \n",
    "    print(\"number_of_parts = 32\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d516202",
   "metadata": {},
   "source": [
    "RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f9225fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test test\n",
      "78.76285910606384\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    gc.collect()\n",
    "    basic_output = model.generate(\n",
    "        ids.long().to(\"cuda\"),\n",
    "        do_sample=True,\n",
    "        num_beams=2,\n",
    "        min_length=max_length,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k = top_k,\n",
    "        top_p = top_p,\n",
    "        repetition_penalty = repetition_penalty,\n",
    "        repetition_penalty_range = repetition_penalty_range,\n",
    "        repetition_penalty_slope = repetition_penalty_slope,\n",
    "        use_cache=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences = 2\n",
    "    ).long()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(tokenizer.decode(basic_output[0]))\n",
    "\n",
    "    print(time.time()  - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4c3ab",
   "metadata": {},
   "source": [
    "# Old Debug VRAM Bandwidth test (2 parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ef675a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.58732867240906\n"
     ]
    }
   ],
   "source": [
    "smalltensor = torch.tensor(0).to(\"cuda\")\n",
    "for j in range(0,32):\n",
    "    for param1 in model.transformer.h[j].parameters():\n",
    "        param1.data = smalltensor\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "start_time = time.time()\n",
    "#0.8gb vram when no model, 5.7gb with full model. 1 loop = 4.9gb transfer to gpu\n",
    "for i in range(50): \n",
    "    smalltensor = torch.tensor(0).to(\"cuda\")\n",
    "    for j in range(0,16):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "            param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "        model.transformer.h[j].to(\"cuda\", non_blocking=True)\n",
    "    for j in range(0,16):\n",
    "        for param1 in model.transformer.h[j].parameters():\n",
    "            param1.data = smalltensor\n",
    "    #gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    for j in range(16,32):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "            param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "        model.transformer.h[j].to(\"cuda\", non_blocking=True)\n",
    "    for j in range(16,32):\n",
    "        for param1 in model.transformer.h[j].parameters():\n",
    "            param1.data = smalltensor\n",
    "    torch.cuda.empty_cache()\n",
    "print(time.time()  - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958453d",
   "metadata": {},
   "source": [
    "# REUSE MEMORY Bandwidth test 32 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37438823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.50335764884949\n"
     ]
    }
   ],
   "source": [
    "smalltensor = torch.tensor(0).to(\"cuda\")\n",
    "for j in range(0,32):\n",
    "    for param1 in model.transformer.h[j].parameters():\n",
    "        param1.data = smalltensor\n",
    "for param1,param2 in zip(model.transformer.h[31].parameters(),model.transformer.extrastorage[31].parameters()):\n",
    "    param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "model.transformer.h[31].to(\"cuda\", non_blocking=True)  \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "start_time = time.time()\n",
    "#0.8gb vram when no model, 5.7gb with full model. 1 loop = 4.9gb transfer to gpu\n",
    "for i in range(50):\n",
    "    for current_number in range(32):\n",
    "        if current_number == 0:\n",
    "            for param1,param2 in zip(model.transformer.h[0].parameters(),model.transformer.h[31].parameters()):\n",
    "                param1.data = param2.data\n",
    "            for param1,param2 in zip(model.transformer.h[current_number].parameters(),model.transformer.extrastorage[current_number].parameters()):\n",
    "                param1.data.copy_(param2.data, non_blocking=True)\n",
    "            model.transformer.h[0].to(\"cuda\", non_blocking=True)\n",
    "            \n",
    "        if current_number >= 1:\n",
    "            for param1,param2 in zip(model.transformer.h[current_number].parameters(),model.transformer.h[current_number-1].parameters()):\n",
    "                param1.data = param2.data\n",
    "            for param1,param2 in zip(model.transformer.h[current_number].parameters(),model.transformer.extrastorage[current_number].parameters()):\n",
    "                param1.data.copy_(param2.data, non_blocking=True)\n",
    "            model.transformer.h[current_number].to(\"cuda\", non_blocking=True)\n",
    "        \n",
    "print(time.time()  - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8ab82c",
   "metadata": {},
   "source": [
    "# REUSE MEMORY 4 PART CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e03626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.776641607284546\n"
     ]
    }
   ],
   "source": [
    "smalltensor = torch.tensor(0).to(\"cuda\")\n",
    "for j in range(0,32):\n",
    "    for param1 in model.transformer.h[j].parameters():\n",
    "        param1.data = smalltensor\n",
    "\n",
    "for j in range(24,32):\n",
    "    for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "        param1.data = param2.data.to(\"cuda\", non_blocking=True)\n",
    "    model.transformer.h[j].to(\"cuda\", non_blocking=True)\n",
    "    \n",
    "start_time = time.time()\n",
    "#0.8gb vram when no model, 5.7gb with full model. 1 loop = 4.9gb transfer to gpu and delete\n",
    "for i in range(50):\n",
    "    #PART 1\n",
    "    #SET STUFF\n",
    "    for j in range(0,8):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.h[j+24].parameters()):\n",
    "            param1.data = param2.data\n",
    "    #CUDASTREAMS\n",
    "    cudastreams = {}\n",
    "    for j in range(0,8):\n",
    "        cudastreams[j] = torch.cuda.Stream()\n",
    "    for j in range(0,8):\n",
    "        with torch.cuda.stream(cudastreams[j]):\n",
    "            for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "                param1.data.copy_(param2.data, non_blocking=True)\n",
    "                pass\n",
    "            model.transformer.h[j].to(\"cuda\", non_blocking=True)\n",
    "    torch.cuda.synchronize()\n",
    "    del cudastreams\n",
    "    \n",
    "    #PART 2\n",
    "    #SET STUFF\n",
    "    for j in range(8,16):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.h[j-8].parameters()):\n",
    "            param1.data = param2.data\n",
    "    #CUDASTREAMS\n",
    "    cudastreams = {}\n",
    "    for j in range(8,16):\n",
    "        cudastreams[j] = torch.cuda.Stream()\n",
    "    for j in range(8,16):\n",
    "        with torch.cuda.stream(cudastreams[j]):\n",
    "            for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "                param1.data.copy_(param2.data, non_blocking=True)\n",
    "                pass\n",
    "            model.transformer.h[j].to(\"cuda\", non_blocking=True)\n",
    "    torch.cuda.synchronize()\n",
    "    del cudastreams\n",
    "    \n",
    "    #PART 3\n",
    "    #SET STUFF\n",
    "    for j in range(16,24):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.h[j-8].parameters()):\n",
    "            param1.data = param2.data\n",
    "    #CUDASTREAMS\n",
    "    cudastreams = {}\n",
    "    for j in range(16,24):\n",
    "        cudastreams[j] = torch.cuda.Stream()\n",
    "    for j in range(16,24):\n",
    "        with torch.cuda.stream(cudastreams[j]):\n",
    "            for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "                param1.data.copy_(param2.data, non_blocking=True)\n",
    "                pass\n",
    "            model.transformer.h[j].to(\"cuda\", non_blocking=True)\n",
    "    torch.cuda.synchronize()\n",
    "    del cudastreams\n",
    "    \n",
    "    #PART 4\n",
    "    #SET STUFF\n",
    "    for j in range(24,32):\n",
    "        for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.h[j-8].parameters()):\n",
    "            param1.data = param2.data\n",
    "    #CUDASTREAMS\n",
    "    cudastreams = {}\n",
    "    for j in range(24,32):\n",
    "        cudastreams[j] = torch.cuda.Stream()\n",
    "    for j in range(24,32):\n",
    "        with torch.cuda.stream(cudastreams[j]):\n",
    "            for param1,param2 in zip(model.transformer.h[j].parameters(),model.transformer.extrastorage[j].parameters()):\n",
    "                param1.data.copy_(param2.data, non_blocking=True)\n",
    "                pass\n",
    "            model.transformer.h[j].to(\"cuda\", non_blocking=True)  \n",
    "    torch.cuda.synchronize()\n",
    "    del cudastreams\n",
    "    \n",
    "print(time.time()  - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
